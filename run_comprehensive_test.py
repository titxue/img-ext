#!/usr/bin/env python3
"""
ç»¼åˆæµ‹è¯•è¿è¡Œå™¨

è¿è¡Œæ‰€æœ‰æµ‹è¯•æ¨¡å—å¹¶ç”Ÿæˆå®Œæ•´çš„HTMLæµ‹è¯•æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- ç‰¹å¾è´¨é‡åˆ†æ
- è®¾å¤‡æ€§èƒ½å¯¹æ¯”
- å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ
- HTMLæŠ¥å‘Šç”Ÿæˆ
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'tests'))

# å¯¼å…¥æµ‹è¯•æ¨¡å—
try:
    from tests.test_performance_benchmark import PerformanceBenchmark
    from tests.test_feature_quality import FeatureQualityAnalyzer
    from tests.test_device_comparison import DevicePerformanceComparator
    from tests.test_visualization import TestVisualizationGenerator
    from tests.test_report_generator import TestReportGenerator
    from feature_extractor import FeatureExtractor
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰æµ‹è¯•æ¨¡å—éƒ½å·²æ­£ç¡®åˆ›å»º")
    sys.exit(1)


class ComprehensiveTestRunner:
    """
    ç»¼åˆæµ‹è¯•è¿è¡Œå™¨
    """
    
    def __init__(self, test_images_dir: str = "tests/test_data/real_images",
                 output_dir: str = "test_results"):
        """
        åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨
        
        Args:
            test_images_dir: æµ‹è¯•å›¾ç‰‡ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
        """
        self.test_images_dir = Path(test_images_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.charts_dir = self.output_dir / "charts"
        self.data_dir = self.output_dir / "data"
        self.charts_dir.mkdir(exist_ok=True)
        self.data_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        self.feature_extractor = FeatureExtractor()
        
        print(f"ğŸš€ ç»¼åˆæµ‹è¯•è¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ æµ‹è¯•å›¾ç‰‡ç›®å½•: {self.test_images_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ–¥ï¸  å½“å‰è®¾å¤‡: {self.feature_extractor.get_device_info()}")
    
    def check_prerequisites(self) -> bool:
        """
        æ£€æŸ¥æµ‹è¯•å‰ææ¡ä»¶
        
        Returns:
            bool: æ˜¯å¦æ»¡è¶³æµ‹è¯•æ¡ä»¶
        """
        print("\nğŸ” æ£€æŸ¥æµ‹è¯•å‰ææ¡ä»¶...")
        
        # æ£€æŸ¥æµ‹è¯•å›¾ç‰‡ç›®å½•
        if not self.test_images_dir.exists():
            print(f"âŒ æµ‹è¯•å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {self.test_images_dir}")
            return False
        
        # æ£€æŸ¥æµ‹è¯•å›¾ç‰‡æ•°é‡
        image_files = list(self.test_images_dir.glob("*.jpg")) + list(self.test_images_dir.glob("*.png"))
        if len(image_files) < 5:
            print(f"âŒ æµ‹è¯•å›¾ç‰‡æ•°é‡ä¸è¶³: {len(image_files)} < 5")
            return False
        
        print(f"âœ… æ‰¾åˆ° {len(image_files)} å¼ æµ‹è¯•å›¾ç‰‡")
        
        # æ£€æŸ¥ä¾èµ–åº“
        try:
            import torch
            import torchvision
            import numpy as np
            import matplotlib.pyplot as plt
            import seaborn as sns
            import sklearn
            import psutil
            import jinja2
            print("âœ… æ‰€æœ‰ä¾èµ–åº“æ£€æŸ¥é€šè¿‡")
        except ImportError as e:
            print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
            return False
        
        return True
    
    def run_performance_benchmark(self) -> Dict[str, Any]:
        """
        è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Returns:
            Dict[str, Any]: æ€§èƒ½æµ‹è¯•ç»“æœ
        """
        print("\nğŸ“Š è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        try:
            benchmark = PerformanceBenchmark(
                feature_extractor=self.feature_extractor,
                test_images_dir=str(self.test_images_dir)
            )
            
            # è¿è¡ŒåŸºå‡†æµ‹è¯•
            results = benchmark.run_comprehensive_benchmark()
            
            # ä¿å­˜ç»“æœ
            results_file = self.data_dir / "performance_benchmark.json"
            benchmark.save_results(str(results_file))
            
            print(f"âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {results_file}")
            return results
            
        except Exception as e:
            print(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return {}
    
    def run_quality_analysis(self) -> Dict[str, Any]:
        """
        è¿è¡Œç‰¹å¾è´¨é‡åˆ†æ
        
        Returns:
            Dict[str, Any]: è´¨é‡åˆ†æç»“æœ
        """
        print("\nğŸ”¬ è¿è¡Œç‰¹å¾è´¨é‡åˆ†æ...")
        
        try:
            analyzer = FeatureQualityAnalyzer(
                feature_extractor=self.feature_extractor,
                test_images_dir=str(self.test_images_dir)
            )
            
            # è¿è¡Œè´¨é‡åˆ†æ
            results = analyzer.run_comprehensive_analysis()
            
            # ä¿å­˜ç»“æœ
            results_file = self.data_dir / "feature_quality.json"
            analyzer.save_results(str(results_file))
            
            print(f"âœ… ç‰¹å¾è´¨é‡åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {results_file}")
            return results
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾è´¨é‡åˆ†æå¤±è´¥: {e}")
            return {}
    
    def run_device_comparison(self) -> Dict[str, Any]:
        """
        è¿è¡Œè®¾å¤‡æ€§èƒ½å¯¹æ¯”
        
        Returns:
            Dict[str, Any]: è®¾å¤‡å¯¹æ¯”ç»“æœ
        """
        print("\nğŸ–¥ï¸ è¿è¡Œè®¾å¤‡æ€§èƒ½å¯¹æ¯”...")
        
        try:
            comparator = DevicePerformanceComparator(
                test_images_dir=str(self.test_images_dir)
            )
            
            # è¿è¡Œè®¾å¤‡å¯¹æ¯”
            results = comparator.run_comprehensive_comparison()
            
            # ä¿å­˜ç»“æœ
            results_file = self.data_dir / "device_comparison.json"
            comparator.save_results(str(results_file))
            
            print(f"âœ… è®¾å¤‡æ€§èƒ½å¯¹æ¯”å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {results_file}")
            return results
            
        except Exception as e:
            print(f"âŒ è®¾å¤‡æ€§èƒ½å¯¹æ¯”å¤±è´¥: {e}")
            return {}
    
    def generate_visualizations(self, benchmark_data: Dict[str, Any],
                              quality_data: Dict[str, Any],
                              device_data: Dict[str, Any]) -> Dict[str, List[str]]:
        """
        ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        
        Args:
            benchmark_data: æ€§èƒ½åŸºå‡†æ•°æ®
            quality_data: è´¨é‡åˆ†ææ•°æ®
            device_data: è®¾å¤‡å¯¹æ¯”æ•°æ®
            
        Returns:
            Dict[str, List[str]]: ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶è·¯å¾„
        """
        print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        try:
            visualizer = TestVisualizationGenerator(output_dir=str(self.charts_dir))
            
            chart_paths = {
                'performance_benchmark': [],
                'feature_quality': [],
                'device_comparison': []
            }
            
            # ç”Ÿæˆæ€§èƒ½åŸºå‡†å›¾è¡¨
            if benchmark_data:
                print("  ğŸ“Š ç”Ÿæˆæ€§èƒ½åŸºå‡†å›¾è¡¨...")
                perf_charts = visualizer.plot_performance_benchmark(benchmark_data)
                chart_paths['performance_benchmark'] = perf_charts
                print(f"    âœ… ç”Ÿæˆäº† {len(perf_charts)} ä¸ªæ€§èƒ½å›¾è¡¨")
            
            # ç”Ÿæˆè´¨é‡åˆ†æå›¾è¡¨
            if quality_data:
                print("  ğŸ”¬ ç”Ÿæˆè´¨é‡åˆ†æå›¾è¡¨...")
                quality_charts = visualizer.plot_feature_quality_analysis(quality_data)
                chart_paths['feature_quality'] = quality_charts
                print(f"    âœ… ç”Ÿæˆäº† {len(quality_charts)} ä¸ªè´¨é‡å›¾è¡¨")
            
            # ç”Ÿæˆè®¾å¤‡å¯¹æ¯”å›¾è¡¨
            if device_data:
                print("  ğŸ–¥ï¸ ç”Ÿæˆè®¾å¤‡å¯¹æ¯”å›¾è¡¨...")
                device_charts = visualizer.plot_device_comparison(device_data)
                chart_paths['device_comparison'] = device_charts
                print(f"    âœ… ç”Ÿæˆäº† {len(device_charts)} ä¸ªè®¾å¤‡å¯¹æ¯”å›¾è¡¨")
            
            total_charts = sum(len(charts) for charts in chart_paths.values())
            print(f"âœ… å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {total_charts} ä¸ªå›¾è¡¨")
            
            return chart_paths
            
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
            return {'performance_benchmark': [], 'feature_quality': [], 'device_comparison': []}
    
    def generate_html_report(self, benchmark_data: Dict[str, Any],
                           quality_data: Dict[str, Any],
                           device_data: Dict[str, Any],
                           chart_paths: Dict[str, List[str]]) -> str:
        """
        ç”ŸæˆHTMLæµ‹è¯•æŠ¥å‘Š
        
        Args:
            benchmark_data: æ€§èƒ½åŸºå‡†æ•°æ®
            quality_data: è´¨é‡åˆ†ææ•°æ®
            device_data: è®¾å¤‡å¯¹æ¯”æ•°æ®
            chart_paths: å›¾è¡¨æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        print("\nğŸ“„ ç”ŸæˆHTMLæµ‹è¯•æŠ¥å‘Š...")
        
        try:
            generator = TestReportGenerator(output_dir=str(self.output_dir))
            
            report_path = generator.generate_report(
                benchmark_data=benchmark_data,
                quality_data=quality_data,
                device_data=device_data,
                chart_paths=chart_paths,
                output_filename="comprehensive_test_report.html"
            )
            
            print(f"âœ… HTMLæµ‹è¯•æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_path}")
            return report_path
            
        except Exception as e:
            print(f"âŒ HTMLæµ‹è¯•æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„ç»¼åˆæµ‹è¯•
        
        Returns:
            Dict[str, Any]: å®Œæ•´æµ‹è¯•ç»“æœ
        """
        print("ğŸ¯ å¼€å§‹è¿è¡Œç»¼åˆæµ‹è¯•å¥—ä»¶")
        print("=" * 60)
        
        start_time = time.time()
        
        # æ£€æŸ¥å‰ææ¡ä»¶
        if not self.check_prerequisites():
            print("âŒ å‰ææ¡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return {}
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        benchmark_data = self.run_performance_benchmark()
        quality_data = self.run_quality_analysis()
        device_data = self.run_device_comparison()
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        chart_paths = self.generate_visualizations(benchmark_data, quality_data, device_data)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        report_path = self.generate_html_report(benchmark_data, quality_data, device_data, chart_paths)
        
        # è®¡ç®—æ€»è€—æ—¶
        total_time = time.time() - start_time
        
        # æ±‡æ€»ç»“æœ
        summary = {
            'test_completed': True,
            'total_time': total_time,
            'benchmark_data': benchmark_data,
            'quality_data': quality_data,
            'device_data': device_data,
            'chart_paths': chart_paths,
            'report_path': report_path,
            'output_dir': str(self.output_dir)
        }
        
        print("\n" + "=" * 60)
        print(f"ğŸ‰ ç»¼åˆæµ‹è¯•å®Œæˆï¼æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®ä¿å­˜åœ¨: {self.data_dir}")
        print(f"ğŸ“ˆ å›¾è¡¨ä¿å­˜åœ¨: {self.charts_dir}")
        if report_path:
            print(f"ğŸ“„ HTMLæŠ¥å‘Š: {report_path}")
        print("=" * 60)
        
        return summary


def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ” å›¾åƒç‰¹å¾æå–å™¨ - ç»¼åˆæµ‹è¯•å¥—ä»¶")
    print("Version: 1.0")
    print("Author: AI Assistant")
    print()
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = ComprehensiveTestRunner()
    
    # è¿è¡Œç»¼åˆæµ‹è¯•
    results = runner.run_comprehensive_test()
    
    if results.get('test_completed', False):
        print("\nâœ… æ‰€æœ‰æµ‹è¯•å·²æˆåŠŸå®Œæˆï¼")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡ä¿¡æ¯
        if results.get('benchmark_data'):
            benchmark = results['benchmark_data']
            if 'single_image_results' in benchmark:
                print(f"ğŸ“Š æµ‹è¯•å›¾ç‰‡æ•°é‡: {len(benchmark['single_image_results'])}")
        
        if results.get('chart_paths'):
            total_charts = sum(len(charts) for charts in results['chart_paths'].values())
            print(f"ğŸ“ˆ ç”Ÿæˆå›¾è¡¨æ•°é‡: {total_charts}")
        
        if results.get('report_path'):
            print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Š: {results['report_path']}")
            print("\nğŸ’¡ æç¤º: å¯ä»¥åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€HTMLæŠ¥å‘ŠæŸ¥çœ‹è¯¦ç»†ç»“æœ")
    else:
        print("\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ä¿¡æ¯")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())