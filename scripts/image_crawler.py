#!/usr/bin/env python3
"""
å›¾ç‰‡çˆ¬è™«è„šæœ¬ - ä»ç½‘ç«™åœ°å›¾ä¸‹è½½äº§å“å›¾ç‰‡
ç”¨äºåˆ›å»ºçœŸå®æµ‹è¯•æ•°æ®é›†
"""

import os
import sys
import requests
import time
from urllib.parse import urlparse
from pathlib import Path
from xml.etree import ElementTree as ET
from typing import List, Optional
import logging
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ImageCrawler:
    """äº§å“å›¾ç‰‡çˆ¬è™«ç±»"""
    
    def __init__(self, output_dir: str = "tests/test_data/real_images", delay: float = 1.0):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            output_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        
    def parse_sitemap(self, sitemap_url: str) -> List[str]:
        """
        è§£æç½‘ç«™åœ°å›¾XMLï¼Œæå–å›¾ç‰‡URL
        
        Args:
            sitemap_url: ç½‘ç«™åœ°å›¾URL
            
        Returns:
            å›¾ç‰‡URLåˆ—è¡¨
        """
        logger.info(f"æ­£åœ¨è§£æç½‘ç«™åœ°å›¾: {sitemap_url}")
        
        try:
            response = self.session.get(sitemap_url, timeout=30)
            response.raise_for_status()
            
            # è§£æXML
            root = ET.fromstring(response.content)
            
            # å®šä¹‰å‘½åç©ºé—´
            namespaces = {
                'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9',
                'image': 'http://www.google.com/schemas/sitemap-image/1.1'
            }
            
            # æå–æ‰€æœ‰å›¾ç‰‡URL
            image_urls = []
            for url_elem in root.findall('.//sitemap:url', namespaces):
                for image_elem in url_elem.findall('.//image:image', namespaces):
                    image_loc = image_elem.find('image:loc', namespaces)
                    if image_loc is not None and image_loc.text:
                        image_urls.append(image_loc.text.strip())
            
            logger.info(f"æ‰¾åˆ° {len(image_urls)} ä¸ªå›¾ç‰‡URL")
            return image_urls
            
        except requests.RequestException as e:
            logger.error(f"è·å–ç½‘ç«™åœ°å›¾å¤±è´¥: {e}")
            return []
        except ET.ParseError as e:
            logger.error(f"è§£æXMLå¤±è´¥: {e}")
            return []
    
    def download_image(self, image_url: str) -> Optional[str]:
        """
        ä¸‹è½½å•ä¸ªå›¾ç‰‡
        
        Args:
            image_url: å›¾ç‰‡URL
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # è§£æURLè·å–æ–‡ä»¶å
            parsed_url = urlparse(image_url)
            filename = os.path.basename(parsed_url.path)
            
            # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œå°è¯•ä»URLå‚æ•°ä¸­è·å–
            if not filename or '.' not in filename:
                filename = f"image_{hash(image_url) % 100000}.jpg"
            
            # ç¡®ä¿æ–‡ä»¶åå®‰å…¨
            filename = "".join(c for c in filename if c.isalnum() or c in '.-_')
            if not filename:
                filename = f"image_{hash(image_url) % 100000}.jpg"
            
            file_path = self.output_dir / filename
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
            if file_path.exists():
                logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                return str(file_path)
            
            # ä¸‹è½½å›¾ç‰‡
            response = self.session.get(image_url, timeout=30, stream=True)
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '')
            if not content_type.startswith('image/'):
                logger.warning(f"éå›¾ç‰‡å†…å®¹ç±»å‹: {content_type} for {image_url}")
                return None
            
            # ä¿å­˜æ–‡ä»¶
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            logger.debug(f"ä¸‹è½½æˆåŠŸ: {filename}")
            return str(file_path)
            
        except requests.RequestException as e:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {image_url}: {e}")
            return None
        except Exception as e:
            logger.error(f"ä¿å­˜å›¾ç‰‡å¤±è´¥ {image_url}: {e}")
            return None
    
    def crawl_images(self, sitemap_url: str, max_images: Optional[int] = None) -> List[str]:
        """
        çˆ¬å–æ‰€æœ‰å›¾ç‰‡
        
        Args:
            sitemap_url: ç½‘ç«™åœ°å›¾URL
            max_images: æœ€å¤§ä¸‹è½½æ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸‹è½½æ‰€æœ‰
            
        Returns:
            æˆåŠŸä¸‹è½½çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # è§£æç½‘ç«™åœ°å›¾
        image_urls = self.parse_sitemap(sitemap_url)
        if not image_urls:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡URL")
            return []
        
        # é™åˆ¶ä¸‹è½½æ•°é‡
        if max_images and len(image_urls) > max_images:
            image_urls = image_urls[:max_images]
            logger.info(f"é™åˆ¶ä¸‹è½½æ•°é‡ä¸º {max_images}")
        
        # ä¸‹è½½å›¾ç‰‡
        downloaded_files = []
        failed_count = 0
        
        logger.info(f"å¼€å§‹ä¸‹è½½ {len(image_urls)} ä¸ªå›¾ç‰‡...")
        
        with tqdm(total=len(image_urls), desc="ä¸‹è½½å›¾ç‰‡") as pbar:
            for i, image_url in enumerate(image_urls):
                # è¯·æ±‚é—´éš”
                if i > 0:
                    time.sleep(self.delay)
                
                file_path = self.download_image(image_url)
                if file_path:
                    downloaded_files.append(file_path)
                else:
                    failed_count += 1
                
                pbar.update(1)
                pbar.set_postfix({
                    'æˆåŠŸ': len(downloaded_files),
                    'å¤±è´¥': failed_count
                })
        
        logger.info(f"ä¸‹è½½å®Œæˆ: æˆåŠŸ {len(downloaded_files)}, å¤±è´¥ {failed_count}")
        return downloaded_files

def main():
    """ä¸»å‡½æ•°"""
    sitemap_url = "https://jettashop.com/sitemap_products_1.xml?from=7351916986606&to=8924092104942"
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ImageCrawler(delay=0.5)  # 0.5ç§’é—´éš”ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
    
    # å¼€å§‹çˆ¬å–ï¼ˆé™åˆ¶å‰50å¼ å›¾ç‰‡ç”¨äºæµ‹è¯•ï¼‰
    downloaded_files = crawler.crawl_images(sitemap_url, max_images=50)
    
    if downloaded_files:
        print(f"\nâœ… æˆåŠŸä¸‹è½½ {len(downloaded_files)} ä¸ªå›¾ç‰‡æ–‡ä»¶")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {crawler.output_dir}")
        print("\nå‰5ä¸ªæ–‡ä»¶:")
        for file_path in downloaded_files[:5]:
            print(f"  - {os.path.basename(file_path)}")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•å›¾ç‰‡")
        sys.exit(1)

if __name__ == "__main__":
    main()