#!/usr/bin/env python3
"""
ç‰¹å¾è´¨é‡åˆ†ææ¨¡å—

åˆ†æå†…å®¹:
- ç‰¹å¾åˆ†å¸ƒç»Ÿè®¡
- ç‰¹å¾ç›¸ä¼¼æ€§åˆ†æ
- èšç±»æ•ˆæœè¯„ä¼°
- ç‰¹å¾é™ç»´å¯è§†åŒ–
- ç‰¹å¾ç¨³å®šæ€§æµ‹è¯•
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from scipy import stats
from typing import List, Dict, Tuple, Any
from pathlib import Path
import json
from datetime import datetime
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from feature_extractor import FeatureExtractor


class FeatureQualityAnalyzer:
    """
    ç‰¹å¾è´¨é‡åˆ†æå™¨
    """
    
    def __init__(self, test_images_dir: str):
        """
        åˆå§‹åŒ–ç‰¹å¾è´¨é‡åˆ†æå™¨
        
        Args:
            test_images_dir: æµ‹è¯•å›¾ç‰‡ç›®å½•
        """
        self.test_images_dir = Path(test_images_dir)
        self.test_images = self._load_test_images()
        self.features = None
        self.image_names = None
        self.results = {}
        
        print(f"åŠ è½½äº† {len(self.test_images)} å¼ æµ‹è¯•å›¾ç‰‡")
    
    def _load_test_images(self) -> List[str]:
        """
        åŠ è½½æµ‹è¯•å›¾ç‰‡è·¯å¾„
        
        Returns:
            List[str]: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        """
        image_extensions = {'.jpg', '.jpeg', '.png', '.webp'}
        images = []
        
        for ext in image_extensions:
            images.extend(self.test_images_dir.glob(f'*{ext}'))
            images.extend(self.test_images_dir.glob(f'*{ext.upper()}'))
        
        return [str(img) for img in images]
    
    def extract_all_features(self, extractor: FeatureExtractor) -> np.ndarray:
        """
        æå–æ‰€æœ‰å›¾ç‰‡çš„ç‰¹å¾
        
        Args:
            extractor: ç‰¹å¾æå–å™¨
            
        Returns:
            np.ndarray: ç‰¹å¾çŸ©é˜µ [N, 512]
        """
        print(f"\nğŸ” æå–æ‰€æœ‰å›¾ç‰‡ç‰¹å¾ (è®¾å¤‡: {extractor.device})")
        
        # æ‰¹é‡æå–ç‰¹å¾
        self.features = extractor.extract_features_batch(self.test_images, batch_size=16)
        self.image_names = [Path(img).name for img in self.test_images]
        
        print(f"âœ… æå–å®Œæˆï¼Œç‰¹å¾çŸ©é˜µå½¢çŠ¶: {self.features.shape}")
        return self.features
    
    def analyze_feature_distribution(self) -> Dict[str, Any]:
        """
        åˆ†æç‰¹å¾åˆ†å¸ƒç»Ÿè®¡
        
        Returns:
            Dict[str, Any]: ç‰¹å¾åˆ†å¸ƒåˆ†æç»“æœ
        """
        print("\nğŸ“Š åˆ†æç‰¹å¾åˆ†å¸ƒ")
        
        if self.features is None:
            raise ValueError("è¯·å…ˆæå–ç‰¹å¾")
        
        # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        mean_features = np.mean(self.features, axis=0)
        std_features = np.std(self.features, axis=0)
        min_features = np.min(self.features, axis=0)
        max_features = np.max(self.features, axis=0)
        
        # æ•´ä½“ç»Ÿè®¡
        overall_stats = {
            'mean': np.mean(self.features),
            'std': np.std(self.features),
            'min': np.min(self.features),
            'max': np.max(self.features),
            'median': np.median(self.features),
            'q25': np.percentile(self.features, 25),
            'q75': np.percentile(self.features, 75)
        }
        
        # ç‰¹å¾ç»´åº¦ç»Ÿè®¡
        dimension_stats = {
            'mean_per_dim': mean_features,
            'std_per_dim': std_features,
            'min_per_dim': min_features,
            'max_per_dim': max_features,
            'range_per_dim': max_features - min_features
        }
        
        # ç¨€ç–æ€§åˆ†æ
        zero_ratio = np.mean(self.features == 0)
        near_zero_ratio = np.mean(np.abs(self.features) < 1e-6)
        
        # æ­£æ€æ€§æ£€éªŒ (å¯¹å‰10ä¸ªç»´åº¦è¿›è¡Œæ£€éªŒ)
        normality_tests = []
        for i in range(min(10, self.features.shape[1])):
            stat, p_value = stats.normaltest(self.features[:, i])
            normality_tests.append({
                'dimension': i,
                'statistic': stat,
                'p_value': p_value,
                'is_normal': p_value > 0.05
            })
        
        results = {
            'overall_stats': overall_stats,
            'dimension_stats': dimension_stats,
            'sparsity': {
                'zero_ratio': zero_ratio,
                'near_zero_ratio': near_zero_ratio
            },
            'normality_tests': normality_tests
        }
        
        print(f"  ğŸ“ˆ æ•´ä½“å‡å€¼: {overall_stats['mean']:.4f} Â± {overall_stats['std']:.4f}")
        print(f"  ğŸ•³ï¸  ç¨€ç–åº¦: {zero_ratio:.2%} (é›¶å€¼), {near_zero_ratio:.2%} (è¿‘é›¶å€¼)")
        print(f"  ğŸ“Š æ­£æ€åˆ†å¸ƒç»´åº¦: {sum(1 for t in normality_tests if t['is_normal'])}/{len(normality_tests)}")
        
        return results
    
    def analyze_feature_similarity(self) -> Dict[str, Any]:
        """
        åˆ†æç‰¹å¾ç›¸ä¼¼æ€§
        
        Returns:
            Dict[str, Any]: ç›¸ä¼¼æ€§åˆ†æç»“æœ
        """
        print("\nğŸ” åˆ†æç‰¹å¾ç›¸ä¼¼æ€§")
        
        if self.features is None:
            raise ValueError("è¯·å…ˆæå–ç‰¹å¾")
        
        # ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        cosine_sim_matrix = cosine_similarity(self.features)
        
        # æ¬§æ°è·ç¦»çŸ©é˜µ
        euclidean_dist_matrix = euclidean_distances(self.features)
        
        # ç›¸ä¼¼åº¦ç»Ÿè®¡ (æ’é™¤å¯¹è§’çº¿)
        mask = ~np.eye(cosine_sim_matrix.shape[0], dtype=bool)
        cosine_similarities = cosine_sim_matrix[mask]
        euclidean_distances_flat = euclidean_dist_matrix[mask]
        
        # æ‰¾å‡ºæœ€ç›¸ä¼¼å’Œæœ€ä¸ç›¸ä¼¼çš„å›¾ç‰‡å¯¹
        max_sim_idx = np.unravel_index(np.argmax(cosine_sim_matrix * (1 - np.eye(len(cosine_sim_matrix)))), 
                                      cosine_sim_matrix.shape)
        min_sim_idx = np.unravel_index(np.argmin(cosine_sim_matrix * (1 - np.eye(len(cosine_sim_matrix)))), 
                                      cosine_sim_matrix.shape)
        
        results = {
            'cosine_similarity': {
                'matrix': cosine_sim_matrix,
                'mean': np.mean(cosine_similarities),
                'std': np.std(cosine_similarities),
                'min': np.min(cosine_similarities),
                'max': np.max(cosine_similarities),
                'median': np.median(cosine_similarities)
            },
            'euclidean_distance': {
                'matrix': euclidean_dist_matrix,
                'mean': np.mean(euclidean_distances_flat),
                'std': np.std(euclidean_distances_flat),
                'min': np.min(euclidean_distances_flat),
                'max': np.max(euclidean_distances_flat),
                'median': np.median(euclidean_distances_flat)
            },
            'most_similar_pair': {
                'images': (self.image_names[max_sim_idx[0]], self.image_names[max_sim_idx[1]]),
                'similarity': cosine_sim_matrix[max_sim_idx]
            },
            'least_similar_pair': {
                'images': (self.image_names[min_sim_idx[0]], self.image_names[min_sim_idx[1]]),
                'similarity': cosine_sim_matrix[min_sim_idx]
            }
        }
        
        print(f"  ğŸ“Š å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦: {results['cosine_similarity']['mean']:.4f} Â± {results['cosine_similarity']['std']:.4f}")
        print(f"  ğŸ“ å¹³å‡æ¬§æ°è·ç¦»: {results['euclidean_distance']['mean']:.4f} Â± {results['euclidean_distance']['std']:.4f}")
        print(f"  ğŸ”— æœ€ç›¸ä¼¼å›¾ç‰‡å¯¹: {results['most_similar_pair']['images'][0]} & {results['most_similar_pair']['images'][1]} (ç›¸ä¼¼åº¦: {results['most_similar_pair']['similarity']:.4f})")
        
        return results
    
    def analyze_clustering_quality(self, n_clusters_range: List[int] = None) -> Dict[str, Any]:
        """
        åˆ†æèšç±»æ•ˆæœ
        
        Args:
            n_clusters_range: èšç±»æ•°é‡èŒƒå›´
            
        Returns:
            Dict[str, Any]: èšç±»è´¨é‡åˆ†æç»“æœ
        """
        print("\nğŸ¯ åˆ†æèšç±»æ•ˆæœ")
        
        if self.features is None:
            raise ValueError("è¯·å…ˆæå–ç‰¹å¾")
        
        if n_clusters_range is None:
            n_clusters_range = list(range(2, min(11, len(self.features) // 2)))
        
        clustering_results = []
        
        for n_clusters in n_clusters_range:
            print(f"  æµ‹è¯• {n_clusters} ä¸ªèšç±»...")
            
            # K-meansèšç±»
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(self.features)
            
            # è®¡ç®—èšç±»è´¨é‡æŒ‡æ ‡
            silhouette_avg = silhouette_score(self.features, cluster_labels)
            calinski_harabasz = calinski_harabasz_score(self.features, cluster_labels)
            
            # è®¡ç®—ç°‡å†…è·ç¦»å’Œç°‡é—´è·ç¦»
            inertia = kmeans.inertia_
            
            clustering_results.append({
                'n_clusters': n_clusters,
                'silhouette_score': silhouette_avg,
                'calinski_harabasz_score': calinski_harabasz,
                'inertia': inertia,
                'cluster_labels': cluster_labels,
                'cluster_centers': kmeans.cluster_centers_
            })
            
            print(f"    è½®å»“ç³»æ•°: {silhouette_avg:.4f}")
            print(f"    Calinski-HarabaszæŒ‡æ•°: {calinski_harabasz:.2f}")
        
        # æ‰¾å‡ºæœ€ä½³èšç±»æ•°
        best_silhouette = max(clustering_results, key=lambda x: x['silhouette_score'])
        best_calinski = max(clustering_results, key=lambda x: x['calinski_harabasz_score'])
        
        results = {
            'clustering_results': clustering_results,
            'best_by_silhouette': {
                'n_clusters': best_silhouette['n_clusters'],
                'score': best_silhouette['silhouette_score']
            },
            'best_by_calinski': {
                'n_clusters': best_calinski['n_clusters'],
                'score': best_calinski['calinski_harabasz_score']
            }
        }
        
        print(f"  ğŸ† æœ€ä½³èšç±»æ•° (è½®å»“ç³»æ•°): {best_silhouette['n_clusters']} (å¾—åˆ†: {best_silhouette['silhouette_score']:.4f})")
        print(f"  ğŸ† æœ€ä½³èšç±»æ•° (Calinski-Harabasz): {best_calinski['n_clusters']} (å¾—åˆ†: {best_calinski['calinski_harabasz_score']:.2f})")
        
        return results
    
    def analyze_dimensionality_reduction(self) -> Dict[str, Any]:
        """
        åˆ†æé™ç»´æ•ˆæœ
        
        Returns:
            Dict[str, Any]: é™ç»´åˆ†æç»“æœ
        """
        print("\nğŸ“‰ åˆ†æé™ç»´æ•ˆæœ")
        
        if self.features is None:
            raise ValueError("è¯·å…ˆæå–ç‰¹å¾")
        
        # PCAé™ç»´
        print("  æ‰§è¡ŒPCAé™ç»´...")
        pca = PCA(n_components=min(50, self.features.shape[0] - 1))
        pca_features = pca.fit_transform(self.features)
        
        # è®¡ç®—ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”
        cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)
        
        # æ‰¾å‡ºè§£é‡Š95%æ–¹å·®æ‰€éœ€çš„ä¸»æˆåˆ†æ•°
        n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1
        n_components_99 = np.argmax(cumulative_variance_ratio >= 0.99) + 1
        
        # t-SNEé™ç»´ (é™åˆ°2Dç”¨äºå¯è§†åŒ–)
        print("  æ‰§è¡Œt-SNEé™ç»´...")
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(self.features) - 1))
        tsne_features = tsne.fit_transform(self.features)
        
        results = {
            'pca': {
                'explained_variance_ratio': pca.explained_variance_ratio_,
                'cumulative_variance_ratio': cumulative_variance_ratio,
                'n_components_95': n_components_95,
                'n_components_99': n_components_99,
                'features_2d': pca_features[:, :2],
                'features_reduced': pca_features
            },
            'tsne': {
                'features_2d': tsne_features
            }
        }
        
        print(f"  ğŸ“Š å‰10ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[:10].sum():.2%}")
        print(f"  ğŸ¯ 95%æ–¹å·®éœ€è¦ {n_components_95} ä¸ªä¸»æˆåˆ†")
        print(f"  ğŸ¯ 99%æ–¹å·®éœ€è¦ {n_components_99} ä¸ªä¸»æˆåˆ†")
        
        return results
    
    def test_feature_stability(self, extractor: FeatureExtractor, num_runs: int = 3) -> Dict[str, Any]:
        """
        æµ‹è¯•ç‰¹å¾ç¨³å®šæ€§
        
        Args:
            extractor: ç‰¹å¾æå–å™¨
            num_runs: è¿è¡Œæ¬¡æ•°
            
        Returns:
            Dict[str, Any]: ç¨³å®šæ€§æµ‹è¯•ç»“æœ
        """
        print(f"\nğŸ”„ æµ‹è¯•ç‰¹å¾ç¨³å®šæ€§ (è¿è¡Œ {num_runs} æ¬¡)")
        
        # é€‰æ‹©å‰5å¼ å›¾ç‰‡è¿›è¡Œç¨³å®šæ€§æµ‹è¯•
        test_images = self.test_images[:min(5, len(self.test_images))]
        
        stability_results = []
        
        for img_path in test_images:
            img_name = Path(img_path).name
            print(f"  æµ‹è¯•å›¾ç‰‡: {img_name}")
            
            # å¤šæ¬¡æå–åŒä¸€å¼ å›¾ç‰‡çš„ç‰¹å¾
            features_runs = []
            for run in range(num_runs):
                features = extractor.extract_features(img_path)
                features_runs.append(features)
            
            # è®¡ç®—ç‰¹å¾çš„ç¨³å®šæ€§
            features_array = np.array(features_runs)
            mean_features = np.mean(features_array, axis=0)
            std_features = np.std(features_array, axis=0)
            
            # è®¡ç®—å˜å¼‚ç³»æ•° (CV = std/mean)
            cv = np.divide(std_features, np.abs(mean_features), 
                          out=np.zeros_like(std_features), where=mean_features!=0)
            
            stability_results.append({
                'image_name': img_name,
                'mean_features': mean_features,
                'std_features': std_features,
                'coefficient_of_variation': cv,
                'mean_cv': np.mean(cv),
                'max_cv': np.max(cv),
                'stable_dimensions': np.sum(cv < 0.01)  # CV < 1%çš„ç»´åº¦æ•°
            })
            
            print(f"    å¹³å‡å˜å¼‚ç³»æ•°: {np.mean(cv):.6f}")
            print(f"    ç¨³å®šç»´åº¦æ•°: {np.sum(cv < 0.01)}/{len(cv)}")
        
        # æ•´ä½“ç¨³å®šæ€§ç»Ÿè®¡
        overall_cv = np.mean([result['mean_cv'] for result in stability_results])
        overall_stable_dims = np.mean([result['stable_dimensions'] for result in stability_results])
        
        results = {
            'num_runs': num_runs,
            'stability_results': stability_results,
            'overall_stability': {
                'mean_coefficient_of_variation': overall_cv,
                'mean_stable_dimensions': overall_stable_dims,
                'stability_ratio': overall_stable_dims / self.features.shape[1] if self.features is not None else 0
            }
        }
        
        print(f"  ğŸ“Š æ•´ä½“å¹³å‡å˜å¼‚ç³»æ•°: {overall_cv:.6f}")
        print(f"  ğŸ¯ å¹³å‡ç¨³å®šç»´åº¦æ¯”ä¾‹: {results['overall_stability']['stability_ratio']:.2%}")
        
        return results
    
    def run_comprehensive_analysis(self, extractor: FeatureExtractor) -> Dict[str, Any]:
        """
        è¿è¡Œå…¨é¢çš„ç‰¹å¾è´¨é‡åˆ†æ
        
        Args:
            extractor: ç‰¹å¾æå–å™¨
            
        Returns:
            Dict[str, Any]: å®Œæ•´çš„åˆ†æç»“æœ
        """
        print("ğŸš€ å¼€å§‹å…¨é¢ç‰¹å¾è´¨é‡åˆ†æ")
        print(f"ğŸ“Š æµ‹è¯•å›¾ç‰‡æ•°é‡: {len(self.test_images)}")
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {extractor.device}")
        
        # æå–æ‰€æœ‰ç‰¹å¾
        self.extract_all_features(extractor)
        
        # è¿è¡Œå„é¡¹åˆ†æ
        all_results = {
            'timestamp': datetime.now().isoformat(),
            'device': str(extractor.device),
            'test_images_count': len(self.test_images),
            'feature_shape': self.features.shape,
            'distribution_analysis': self.analyze_feature_distribution(),
            'similarity_analysis': self.analyze_feature_similarity(),
            'clustering_analysis': self.analyze_clustering_quality(),
            'dimensionality_analysis': self.analyze_dimensionality_reduction(),
            'stability_analysis': self.test_feature_stability(extractor)
        }
        
        # ä¿å­˜ç»“æœ
        self.results = all_results
        return all_results
    
    def save_results(self, output_path: str = None) -> str:
        """
        ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"feature_quality_analysis_{timestamp}.json"
        
        # å¤„ç†numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, dict):
                return {key: convert_numpy(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            else:
                return obj
        
        serializable_results = convert_numpy(self.results)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        return output_path


if __name__ == "__main__":
    # è¿è¡Œç‰¹å¾è´¨é‡åˆ†æ
    test_images_dir = "test_data/real_images"
    
    if not os.path.exists(test_images_dir):
        print(f"âŒ æµ‹è¯•å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {test_images_dir}")
        exit(1)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = FeatureQualityAnalyzer(test_images_dir)
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = FeatureExtractor(device='auto')
    
    # è¿è¡Œåˆ†æ
    results = analyzer.run_comprehensive_analysis(extractor)
    
    # ä¿å­˜ç»“æœ
    analyzer.save_results()
    
    print("\nğŸ‰ ç‰¹å¾è´¨é‡åˆ†æå®Œæˆ!")